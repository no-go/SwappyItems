# Stand 2020-08-04

Ich habe nun nach 4 Monaten nochmal in die Map und Routing Sachen
reingeschaut. Das war ganz gut, da mir so klar wurde, wo ich schlecht
bzw. nicht gut nachvollziehbar programmiert und dokumenntiert habe.

Ich habe ca ende März unbewusst alles als gescheitert abgestempelt,
sonst hätte ich es ja so lange nicht liegen lassen. Dann kam
Corona-Blues und viel Unzufriedenheit mit dem Job dazu.

An folgenden Punkten scheiterte ich damals:

- Mangel an "Grifffestigkeit" des Themas und der Motivation der
  möglichen Masterarbeit
- zu viele kleine und große Baustellen bei dem Themenkomplex
- nichts, wo ich 100% hinter stehe und gut erklähren könnte, warum
  mich Herr Wanke betreuen sollte
  
Konkret:

- ich scheitere bei der Begründung (im Detail!), warum man das Caching
  der großen Datenmengen nicht dem Betriebsystem überlassen sollte
- ich scheitere an der Komplexität der OpenStreetMap-Daten, um diese
  im Umfang "europa" sinvoll zu reduzieren.
- hatte mit über einem Tag rechnen (und nahe meinem Hardware-Limit)
  bisher nur NRW verarbeiten können = ein gerichteter Graph mit
  Positionen, Abständen aber ohne(!!) Abbiegebedingungen und daher
  sinnlos
- trotz viel Aufwand in ein eigenes Caching und Ext. Speicherstruktur
  ist der "Reduzierungsaufwand" zeitlich weiterhin extrem hoch
- trotz einer umfangreichen Literaturrecherche findet sich keine
  brauchbare Beschreibung einer Ext. Priority Queue, die einen Teil
  im Speicher hat oder mit großen Datenmengen arbeitet und gleichzeitig
  eine Anpassung der Priorität bietet

Alles das ist weit weit weit von dem entfernt, wo du sagtest: Da ließt
man sich jeweils in 1-2 Paper ein und am Ende will ich einen Vergleich
und Implementierungen von den Datenstrukturen und Algorithemen dieser
Paper haben :-S

Bisher laufen alle Sachen, die ich in der Literatur gefunden habe, auf
diese Punkte und Extreme hinaus:

- um einen Graphen zu haben, dessen Datenmenge eine Ext. Priority Queue
  beim Routing rechtfertigt, muss ich schon Deutschland oder Europa aus
  Roh-Daten einlesen, um daraus einen entsprechend großen Graphen zu
  bilden. 
- um Deutschland oder Europa als Roh-Daten zu einem Graphen verarbeiten
  zu können, muss ich derzeit noch einen enormen Hardware- und
  Zeitaufwand betreiben. In dem Fall habe ich (oder die
  wissenschaftliche Veröffentlichung, die ich laß) bereits genug
  Hardware, so dass eine Ext. Priority Queue nicht nötig ist, sollte
  man hinterher Routing machen müssen
- Die wissenschaftlichen Veröffentlichungen zum Routing, die ich laß,
  haben alle mit bereits reduzierten roh-Daten gearbeitet um zum Teil
  beschrieben, wie sich diese noch weiter reduzieren lassen, dass keine
  Ext. Datenstrukturen bemüht werden müssen. Dies jedoch zum Leidwesen
  der Brauchbarkeit: man hat dann zwar einen minimalen Spannbaum, jedoch
  ist dieser nicht mehr mit einer realen Karte in Verbindung zu bringen
  und Abbiege-Regeln oder Tempo-Limits fehlen.
- Die wissenschaftlichen Veröffentlichungen zu Priority Queues, die ich
  laß, haben entweder keinen Fokus auf Bigdata und Externe
  Datenstrukturen oder beschreiben fürs Routing unbrauchbare Lösungen,
  in denen man die Priorität eines bestimmten Datensatzes (wahlfreier
  Zugriff) nicht ändern kann. 

In meinen damals letzten Überlegungen stolperte ich über folgendes:

- warum nicht 2 sortierte Listen nehmen:
  - sortiert nach Priority und je Priority nach Key
  - sortiert nach Key und darin abgelegt die Priority + Daten
- wie lege ich das im RAM ab, um nicht oft zu springen?
- wie füge ich eine Sortierung hinzu, so dass alte Daten (selten
  genutzte Keys) extern gespeichert werden?
- Wie manchen es andere? Antwort: Sobald "Bigdata" im Spiel ist, beginnt
  die Framework- und vernetzte Hardware-Schlacht!

Im Grunde finde ich die Hürden an die nötige Datenstruktur(en) zu groß:

- zur Verarbeitung einer Datenmenge, wo bereits die Anzahl der
  Primärschlüssel nicht in den Arbeitsspeicher passt
- eine Datenstruktur, die die Daten von seltenen Key-Zugriffen extern
  auslagert
- eine Datenstruktur, die nach einer Priorität sortiert ist
- eine Datenstruktur, die niedrige Prioritäten extern auslagert
- eine Datenstruktur, bei der man wahlfreien Zugriff hat, um eine
  Priorität ändern zu können
- eine Datenstruktur, die große Daten auf einem Rechner verarbeiten
  kann (z.B. SSD 100x größer als der RAM und Daten ca 1/10 der SSD)
- eine Datenstruktur, die optimal die 2D Eigenschaft/Nähe nutzen um
  ggf. Prefetching der Daten zu machen oder diese optimal im RAM
  platziert für nur wenige Sprünge (ggf. Cache-Effekte nutzen)

Daher hab ich wohl Ende März innerlich kapituliert!

Aktuell lese ich mich jedoch nochmal in den Code ein, um:

- am Beispiel "Krefeld" die Abbiege-Bedingungen zu verarbeiten
- meine letzten Ideen bzgl. einer Ext. Prio Queue nochmal
  nachvollziehen und zu ende bringen zu können

Zwischendurch hatte ich die Idee, man könne doch man krefeld einlesen
und aus Krefeld eine "brauchbare" Einbahnstraßen-Struktur machen.
Die Idee dabei ist, dass aus der 2. Spur ein Radweg wird. Ziel ist
es, 50% der Straßen zu Radwegen zu machen und daraus dann wieder
eine neue OpenStreetMap-Karte zu generieren. Allerdings sollte die
Karte so erzeugt werden, dass Ampeln reduziert werden können (mehr
Kreisverkehre oder Regionen/Geraden, wo Autos nicht Kreuzen) und
Autos weiterhin mit wenig Umwegen nah ans Ziel kommen.